{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_access import read_in_data, read_llm_output_data, read_output_data\n",
    "import pandas as pd\n",
    "import os\n",
    "import xlsxwriter\n",
    "# api key:\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"craigslist_data_wrangler\", \"crime_data_wrangler\", \"potters_wheel_divide\", \"potters_wheel_fold\" ,\n",
    "                     \"potters_wheel_fold_2\", \"potters_wheel_merge_split\", \"potters_wheel_split_fold\", \"potters_wheel_unfold\", \n",
    "                     \"potters_wheel_unfold2\", \"proactive_wrangling_fold\", \"proactive_wrangling_complex\", \"reshape_table_structure_data_wrangler\"]\n",
    "missing_list = [9, 14, 16, 20, 21, 23, 25, 31, 32, 35, 38, 39, 42,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc():\n",
    "    acc = []\n",
    "    for j in name_list:\n",
    "        \n",
    "        acc_1_5 = [0,0,0]\n",
    "        for i in range(1,6):\n",
    "            print(j,i)\n",
    "            path = '../data/foofah/foofah/exp0_'+str(j) + '_'+ str(i)+ '.txt'\n",
    "            input_data, test_data = read_in_data(path)\n",
    "            gpt_3_5 = read_llm_output_data('../output/llama-2-7b-chat-hf/foofah/new_output_data_0_'+str(j) + '_'+ str(i)+ '.json')\n",
    "            # gpt_4_0 = read_llm_output_data('../output/chat_gpt_4.0/foofah/new_output_data_0_'+str(j) + '_'+ str(i)+ '.json')\n",
    "            # if os.path.isfile('../output/foofah/foofah/exp0_results_'+str(j) + '_'+ str(i)+ '.txt'):\n",
    "            #     foofah = read_output_data('../output/foofah/foofah/exp0_results_'+str(j) + '_'+ str(i)+ '.txt')\n",
    "            # else:\n",
    "            #     foofah = [[]]\n",
    "            for p, output_data in enumerate([gpt_3_5]):\n",
    "            #compare output_data and test_data\n",
    "                acc_temp = 0\n",
    "                    # print(j,i)\n",
    "                for k in range(min(len(output_data),len(test_data[1]))):\n",
    "                    for m in range(min(len(output_data[k]),len(test_data[1][k]))):\n",
    "                        if output_data[k][m] == test_data[1][k][m]:\n",
    "                            acc_temp += 1\n",
    "                acc_1_5[p] += (acc_temp/(len(test_data[1])*len(test_data[1][0])))/5\n",
    "                acc_1_5[1] = max(acc_1_5[1], acc_temp/(len(test_data[1])*len(test_data[1][0])))\n",
    " \n",
    "\n",
    "        acc.append(acc_1_5)\n",
    "    return(acc)\n",
    "                \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craigslist_data_wrangler 1\n",
      "craigslist_data_wrangler 2\n",
      "craigslist_data_wrangler 3\n",
      "craigslist_data_wrangler 4\n",
      "craigslist_data_wrangler 5\n",
      "crime_data_wrangler 1\n",
      "crime_data_wrangler 2\n",
      "crime_data_wrangler 3\n",
      "crime_data_wrangler 4\n",
      "crime_data_wrangler 5\n",
      "potters_wheel_divide 1\n",
      "potters_wheel_divide 2\n",
      "potters_wheel_divide 3\n",
      "potters_wheel_divide 4\n",
      "potters_wheel_divide 5\n",
      "potters_wheel_fold 1\n",
      "potters_wheel_fold 2\n",
      "potters_wheel_fold 3\n",
      "potters_wheel_fold 4\n",
      "potters_wheel_fold 5\n",
      "potters_wheel_fold_2 1\n",
      "potters_wheel_fold_2 2\n",
      "potters_wheel_fold_2 3\n",
      "potters_wheel_fold_2 4\n",
      "potters_wheel_fold_2 5\n",
      "potters_wheel_merge_split 1\n",
      "potters_wheel_merge_split 2\n",
      "potters_wheel_merge_split 3\n",
      "potters_wheel_merge_split 4\n",
      "potters_wheel_merge_split 5\n",
      "potters_wheel_split_fold 1\n",
      "potters_wheel_split_fold 2\n",
      "potters_wheel_split_fold 3\n",
      "potters_wheel_split_fold 4\n",
      "potters_wheel_split_fold 5\n",
      "potters_wheel_unfold 1\n",
      "potters_wheel_unfold 2\n",
      "potters_wheel_unfold 3\n",
      "potters_wheel_unfold 4\n",
      "potters_wheel_unfold 5\n",
      "potters_wheel_unfold2 1\n",
      "potters_wheel_unfold2 2\n",
      "potters_wheel_unfold2 3\n",
      "potters_wheel_unfold2 4\n",
      "potters_wheel_unfold2 5\n",
      "proactive_wrangling_fold 1\n",
      "proactive_wrangling_fold 2\n",
      "proactive_wrangling_fold 3\n",
      "proactive_wrangling_fold 4\n",
      "proactive_wrangling_fold 5\n",
      "proactive_wrangling_complex 1\n",
      "proactive_wrangling_complex 2\n",
      "proactive_wrangling_complex 3\n",
      "proactive_wrangling_complex 4\n",
      "proactive_wrangling_complex 5\n",
      "reshape_table_structure_data_wrangler 1\n",
      "reshape_table_structure_data_wrangler 2\n",
      "reshape_table_structure_data_wrangler 3\n",
      "reshape_table_structure_data_wrangler 4\n",
      "reshape_table_structure_data_wrangler 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 0, 0],\n",
       " [0.0, 0, 0],\n",
       " [0.09393939393939393, 0.30303030303030304, 0],\n",
       " [0.0, 0, 0],\n",
       " [0.0, 0, 0],\n",
       " [0.15, 0.75, 0],\n",
       " [0.06, 0.3, 0],\n",
       " [0.0, 0, 0],\n",
       " [0.07777777777777778, 0.2222222222222222, 0],\n",
       " [0.027777777777777776, 0.08333333333333333, 0],\n",
       " [0.0, 0, 0],\n",
       " [0.03571428571428571, 0.10714285714285714, 0]]"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = calculate_acc()\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(acc)\n",
    "df.to_excel('acc.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 7b chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030612244897959183,\n",
       " 0.0989010989010989,\n",
       " 0.10714285714285714,\n",
       " 0.11688311688311688,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.005494505494505495,\n",
       " 0.44047619047619047,\n",
       " 0.38961038961038963,\n",
       " 0.38571428571428573,\n",
       " 0.044444444444444446,\n",
       " 0.0,\n",
       " 0.1111111111111111,\n",
       " 0.0,\n",
       " 0.2222222222222222,\n",
       " 0.03529411764705882,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.17647058823529413]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_7b_chat_acc = acc\n",
    "llama2_7b_chat_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2388125724260178"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(llama2_7b_chat_acc)/len(llama2_7b_chat_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat GPT Accuracy 3.5 turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030612244897959183,\n",
       " 0.42857142857142855,\n",
       " 0.42857142857142855,\n",
       " 0.42857142857142855,\n",
       " 0.42857142857142855,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.03571428571428571,\n",
       " 0.18681318681318682,\n",
       " 0.13690476190476192,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 0.058823529411764705,\n",
       " 0.11764705882352941,\n",
       " 0.11764705882352941,\n",
       " 0.11764705882352941]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_gpt_acc = acc\n",
    "chat_gpt_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34164760776105313"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(chat_gpt_acc)/len(chat_gpt_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy code and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 'potters_wheel_unfold'\n",
    "j = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update i j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"craigslist_data_wrangler\", \"crime_data_wrangler\", \"potters_wheel_divide\", \"potters_wheel_fold\" ,\n",
    "                     \"potters_wheel_fold_2\", \"potters_wheel_merge_split\", \"potters_wheel_split_fold\", \"potters_wheel_unfold\", \n",
    "                     \"potters_wheel_unfold2\", \"proactive_wrangling_fold\", \"proactive_wrangling_complex\", \"reshape_table_structure_data_wrangler\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "j = 0\n",
    "i = name_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape_table_structure_data_wrangler 5\n"
     ]
    }
   ],
   "source": [
    "j += 1\n",
    "if j == 6:\n",
    "    j = 1\n",
    "    idx += 1\n",
    "    i = name_list[idx]\n",
    "print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 1\n"
     ]
    }
   ],
   "source": [
    "j += 1\n",
    "if j == 6:\n",
    "    j = 1\n",
    "    i += 1\n",
    "print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_data):\n",
    "    # Initialize empty lists to store the transformed data\n",
    "    outputs = []\n",
    "    for year in range(len(input_data)):\n",
    "        # Calculate the mean of the values for each month\n",
    "        mean = sum(input_data[year][month] for month in range(1, 7)) / len(input_data[year])\n",
    "        \n",
    "        # Add the mean to the list of transformed data\n",
    "        outputs.append([mean, input_data[year][1], input_data[year][2], input_data[year][3], input_data[year][4], input_data[year][5]])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape_table_structure_data_wrangler 5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[835], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/foofah/foofah/exp0_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m input_data, test_data \u001b[38;5;241m=\u001b[39m read_in_data(path)\n\u001b[0;32m----> 9\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../output/chat_gpt_4.0/foofah/new_output_data_0_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file: \n\u001b[1;32m     11\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(r, file)\n",
      "Cell \u001b[0;32mIn[834], line 6\u001b[0m, in \u001b[0;36mtransform_data\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_data)):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Calculate the mean of the values for each month\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43myear\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_data[year])\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Add the mean to the list of transformed data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend([mean, input_data[year][\u001b[38;5;241m1\u001b[39m], input_data[year][\u001b[38;5;241m2\u001b[39m], input_data[year][\u001b[38;5;241m3\u001b[39m], input_data[year][\u001b[38;5;241m4\u001b[39m], input_data[year][\u001b[38;5;241m5\u001b[39m]])\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "j += 1\n",
    "if j == 6:\n",
    "    j = 1\n",
    "    idx += 1\n",
    "    i = name_list[idx]\n",
    "print(i,j)\n",
    "path = '../data/foofah/foofah/exp0_'+ str(i) + '_'+ str(j) + '.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "r = transform_data(test_data[0])\n",
    "with open('../output/chat_gpt_4.0/foofah/new_output_data_0_'+str(i) + '_'+ str(j)+'.json', \"w\") as file: \n",
    "    json.dump(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/foofah/foofah/exp0_'+ str(i) + '_'+ str(j) + '.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "with open('../output/chat_gpt_4.0/foofah/new_output_data_0_'+str(i) + '_'+ str(j)+'.json', \"w\") as file: \n",
    "    json.dump([[]], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/foofah/foofah/exp0_'+ str(2) + '_'+ str(3) + '.txt'\n",
    "input_data, test_data = read_in_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 3, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[403], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../output/Llama-2-7b-chat-hf/foofah/output_chat_llama2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 3, saw 4\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "path = \"../output/Llama-2-7b-chat-hf/foofah/output_chat_llama2.csv\"\n",
    "df = pandas.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to LLAma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 26\n",
    "jj = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 5\n"
     ]
    }
   ],
   "source": [
    "jj += 1\n",
    "if jj == 6:\n",
    "    jj = 1\n",
    "    ii += 1\n",
    "print(ii,jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_dataset):\n",
    "    output_dataset = []\n",
    "    headers = input_dataset[0]\n",
    "    for row in input_dataset[1:]:\n",
    "        name = row[0]\n",
    "        for i, value in enumerate(row[1:], start=1):\n",
    "            if value:\n",
    "                output_dataset.append([name, headers[i], value])\n",
    "    return output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 5\n"
     ]
    }
   ],
   "source": [
    "jj += 1\n",
    "if jj == 6:\n",
    "    jj = 1\n",
    "    ii += 1\n",
    "print(ii,jj)\n",
    "path = '../data/foofah/foofah/exp0_'+ str(ii) + '_'+ str(jj) + '.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "r = transform_data(test_data[0])\n",
    "with open('../output/Llama-2-7b-chat-hf/foofah/output_data_0_'+str(ii) + '_'+ str(jj)+'.json', \"w\") as file: \n",
    "    json.dump(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/foofah/foofah/exp0_'+ str(ii) + '_'+ str(jj) + '.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "#r = transform_data(test_data[0])\n",
    "with open('../output/Llama-2-7b-chat-hf/foofah/output_data_0_'+str(ii) + '_'+ str(jj)+'.json', \"w\") as file: \n",
    "    json.dump([[]], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACC save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output/Llama-2-7b-chat-hf/accuracy.json', \"w\") as file: \n",
    "    json.dump(acc, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
